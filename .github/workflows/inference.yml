name: Test Inference

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/inference.py'
      - 'artifacts/'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/inference.py'
  workflow_dispatch:
    inputs:
      model_path:
        description: 'Path to model artifact'
        required: false
        default: 'artifacts/model.pkl'

jobs:
  inference-test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: training-artifacts-${{ github.run_number }}
        path: artifacts/
      continue-on-error: true
        
    - name: Check if model exists
      id: check-model
      run: |
        if [ -f "artifacts/model.pkl" ]; then
          echo "model_exists=true" >> $GITHUB_OUTPUT
          echo "✅ Model artifact found"
        else
          echo "model_exists=false" >> $GITHUB_OUTPUT
          echo "⚠️ No model artifact found, will create dummy model for testing"
        fi
        
    - name: Create dummy model for testing
      if: steps.check-model.outputs.model_exists == 'false'
      run: |
        python -c "
        import numpy as np
        import joblib
        import os
        from sklearn.ensemble import RandomForestClassifier
        
        # Create dummy model
        X_dummy = np.random.randn(100, 5)
        y_dummy = np.random.randint(0, 2, 100)
        
        model = RandomForestClassifier(n_estimators=5, random_state=42)
        model.fit(X_dummy, y_dummy)
        
        # Save model
        os.makedirs('artifacts', exist_ok=True)
        joblib.dump(model, 'artifacts/model.pkl')
        
        # Save dummy metrics
        import json
        metrics = {
            'accuracy': 0.85,
            'classification_report': 'dummy report'
        }
        with open('artifacts/metrics.json', 'w') as f:
            json.dump(metrics, f)
            
        print('✅ Dummy model created for testing')
        "
        
    - name: Test inference functionality
      run: |
        cd src
        python -c "
        import sys
        import os
        import numpy as np
        
        # Add current directory to path
        sys.path.append(os.path.dirname(os.path.abspath('.')))
        
        from inference import ModelPredictor
        
        # Test predictor initialization
        predictor = ModelPredictor('../artifacts/model.pkl')
        print('✅ ModelPredictor initialized successfully')
        
        # Test prediction
        test_data = np.random.randn(10, 5)
        predictions = predictor.predict(test_data)
        print(f'✅ Made {len(predictions)} predictions')
        
        # Test probability prediction
        probabilities = predictor.predict_proba(test_data)
        print(f'✅ Made {len(probabilities)} probability predictions')
        
        # Test with different input formats
        list_data = [[1, 2, 3, 4, 5]]
        predictions_list = predictor.predict(list_data)
        print('✅ List input prediction successful')
        
        single_data = [1, 2, 3, 4, 5]
        predictions_single = predictor.predict(single_data)
        print('✅ Single sample prediction successful')
        
        print('🎯 All inference tests passed!')
        "
        
    - name: Test inference performance
      run: |
        cd src
        python -c "
        import time
        import numpy as np
        import sys
        import os
        
        sys.path.append(os.path.dirname(os.path.abspath('.')))
        from inference import ModelPredictor
        
        # Initialize predictor
        predictor = ModelPredictor('../artifacts/model.pkl')
        
        # Test batch prediction performance
        batch_sizes = [1, 10, 100, 1000]
        
        for batch_size in batch_sizes:
            test_data = np.random.randn(batch_size, 5)
            
            start_time = time.time()
            predictions = predictor.predict(test_data)
            end_time = time.time()
            
            latency = (end_time - start_time) * 1000  # Convert to milliseconds
            throughput = batch_size / (end_time - start_time)
            
            print(f'Batch size {batch_size}: {latency:.2f}ms latency, {throughput:.0f} predictions/sec')
            
        print('✅ Performance testing completed')
        "
        
    - name: Test error handling
      run: |
        cd src
        python -c "
        import sys
        import os
        import numpy as np
        
        sys.path.append(os.path.dirname(os.path.abspath('.')))
        from inference import ModelPredictor
        
        predictor = ModelPredictor('../artifacts/model.pkl')
        
        # Test with invalid data
        try:
            predictor.predict([])
            print('❌ Should have raised error for empty data')
        except Exception as e:
            print(f'✅ Correctly handled empty data: {e}')
            
        # Test with wrong dimensions
        try:
            predictor.predict(np.random.randn(5))  # 1D array
            print('✅ Handled 1D array correctly')
        except Exception as e:
            print(f'✅ Correctly handled 1D array: {e}')
            
        print('✅ Error handling tests completed')
        "
        
    - name: Validate model artifacts
      run: |
        python -c "
        import os
        import json
        import joblib
        
        # Check model file
        model_path = 'artifacts/model.pkl'
        assert os.path.exists(model_path), f'Model file not found: {model_path}'
        
        # Load and validate model
        model = joblib.load(model_path)
        assert hasattr(model, 'predict'), 'Model missing predict method'
        assert hasattr(model, 'predict_proba'), 'Model missing predict_proba method'
        
        # Check metrics file
        metrics_path = 'artifacts/metrics.json'
        if os.path.exists(metrics_path):
            with open(metrics_path, 'r') as f:
                metrics = json.load(f)
            assert 'accuracy' in metrics, 'Metrics missing accuracy'
            print(f'Model accuracy: {metrics[\"accuracy\"]:.4f}')
        
        # Check config file
        config_path = 'artifacts/config.json'
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
            print('Model config loaded successfully')
        
        print('✅ Model artifacts validation passed!')
        "
        
    - name: Create inference summary
      run: |
        echo "## 🎯 Inference Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ✅ All inference tests passed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Test Results" >> $GITHUB_STEP_SUMMARY
        echo "- Model loading: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Prediction functionality: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Probability prediction: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Input format handling: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Error handling: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Performance testing: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Artifact validation: ✅" >> $GITHUB_STEP_SUMMARY 