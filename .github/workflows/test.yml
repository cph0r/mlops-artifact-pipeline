name: Run Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black
        
    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        
    - name: Check code formatting with black
      run: |
        black --check src/ tests/
        
    - name: Create test directories
      run: |
        mkdir -p data/raw data/processed artifacts
        
    - name: Generate test data
      run: |
        python -c "
        import numpy as np
        import pandas as pd
        from sklearn.datasets import make_classification
        
        # Generate small test dataset
        X, y = make_classification(n_samples=100, n_features=5, n_informative=3, 
                                 n_redundant=1, n_classes=2, random_state=42)
        
        # Create DataFrame
        feature_names = [f'feature_{i}' for i in range(X.shape[1])]
        df = pd.DataFrame(X, columns=feature_names)
        df['target'] = y
        
        # Split into train/test
        train_df = df.sample(frac=0.8, random_state=42)
        test_df = df.drop(train_df.index)
        
        # Save data
        train_df.to_csv('data/processed/train.csv', index=False)
        test_df.to_csv('data/processed/test.csv', index=False)
        
        print('Test data generated successfully!')
        "
        
    - name: Run tests with pytest
      run: |
        cd tests
        python -m pytest test_train.py -v --cov=../src --cov-report=xml --cov-report=term-missing
        
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./tests/coverage.xml
        flags: unittests
        name: codecov-umbrella
        
    - name: Test model training
      run: |
        cd src
        python -c "
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.abspath('.')))
        
        # Test basic imports
        from train import train_model, evaluate_model
        from utils import setup_logging, load_config, save_artifacts
        
        print('âœ… All imports successful!')
        "
        
    - name: Test inference module
      run: |
        cd src
        python -c "
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.abspath('.')))
        
        # Test inference imports
        from inference import ModelPredictor
        
        print('âœ… Inference module imports successful!')
        "
        
    - name: Check for security vulnerabilities
      run: |
        pip install safety
        safety check
        
    - name: Test configuration loading
      run: |
        python -c "
        import json
        import os
        
        # Test config file exists and is valid JSON
        config_path = 'config/config.json'
        assert os.path.exists(config_path), f'Config file not found: {config_path}'
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Check required keys
        required_keys = ['data', 'model', 'training', 'artifacts']
        for key in required_keys:
            assert key in config, f'Missing required config key: {key}'
        
        print('âœ… Configuration validation passed!')
        "
        
    - name: Create test summary
      run: |
        echo "## ðŸ§ª Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Python Version:** ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** âœ… All tests passed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Test Coverage" >> $GITHUB_STEP_SUMMARY
        echo "- Unit tests: âœ…" >> $GITHUB_STEP_SUMMARY
        echo "- Integration tests: âœ…" >> $GITHUB_STEP_SUMMARY
        echo "- Code quality checks: âœ…" >> $GITHUB_STEP_SUMMARY
        echo "- Security checks: âœ…" >> $GITHUB_STEP_SUMMARY 